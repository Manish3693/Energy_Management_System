{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-01 21:23:26.728684: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-01 21:23:29.693266: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "from stable_baselines3 import DQN,A2C,PPO,SAC\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives:\n",
    "\n",
    "The objective of the project is to design, train and evaluate a framework for an agent in a low-energy environment. It seeks to efficiently manage the heat and air in the room to reduce energy consumption and improve occupant comfort. \n",
    "\n",
    "Leveraging reinforcement learning, the goal is to train agents to dynamically adjust controls based on factors such as occupancy, solar radiation, energy availability, and so on. \n",
    "\n",
    "Ultimately, the project aims to help achieve optimal energy without compromising comfort, as likewise sustainable building design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This environment is designed to be able to simulate the house of a consumer/user and train an agent specific to all the rooms in the house to utilize energy as efficiently as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "import gym.spaces as spaces\n",
    "\n",
    "class EnergySavingEnv_disc(gym.Env):\n",
    "    def __init__(self, num_bedrooms=3, num_people_per_bedroom=[1,2,1], num_people_in_living_room=2):\n",
    "        self.num_bedrooms = num_bedrooms\n",
    "        self.num_people_per_bedroom = num_people_per_bedroom if num_people_per_bedroom is not None else [2] * num_bedrooms\n",
    "        self.num_people_in_living_room = num_people_in_living_room\n",
    "\n",
    "        # Define the action space\n",
    "        self.action_space = spaces.Discrete(28)  # Total number of actions: 7 temperature adjustments * 4 airflow adjustments\n",
    "        \n",
    "        # Define the observation space with normalized ranges\n",
    "        num_rooms = num_bedrooms + 1  # Include living room as well\n",
    "        self.observation_space = spaces.Box(low=-1, high=1, shape=(num_rooms+1,1), dtype=np.float32)\n",
    "\n",
    "        # Define the initial temperature (random between 24 and 26 degrees Celsius) for each room\n",
    "        self.initial_temperatures = np.random.uniform(24, 26, size=(num_rooms,))\n",
    "\n",
    "        # Set the current temperature to the initial temperature\n",
    "        self.temperatures = self.initial_temperatures.copy()\n",
    "\n",
    "        # Initialize energy level for the whole house\n",
    "        self.energy_level = np.array([120])\n",
    "\n",
    "        # Define the ideal temperature for each room\n",
    "        self.ideal_temperatures = [22] * num_rooms  # Assuming ideal temperature of 22 for all rooms\n",
    "\n",
    "        # Define the resource manager properties\n",
    "        self.energy_capacity = 250\n",
    "        self.energy_threshold = 50  # Energy threshold for energy-saving mode\n",
    "        self.energy_usage_factor = 2  # Factor to control energy usage for temperature adjustment\n",
    "        self.base_airflow = 0  # Base airflow without energy consumption\n",
    "        self.max_airflow = 3  # Maximum airflow that can be achieved with energy consumption\n",
    "        self.airflow_energy_consumption = 0.5  # Energy consumption per unit increase in airflow\n",
    "\n",
    "        # Define the number of time steps for the resource manager\n",
    "        self.time_steps = 100\n",
    "\n",
    "        # Initialize the current time step\n",
    "        self.current_step = 0\n",
    "\n",
    "        self.cleaning_energy_consumption = 1  # Energy consumed during home cleaning\n",
    "\n",
    "        # Define the frequency of home cleaning (once a day/episode)\n",
    "        self.cleaning_frequency = 1\n",
    "\n",
    "        # Add solar energy parameters\n",
    "        self.solar_panel_efficiency = 0.2  # Efficiency of solar panels\n",
    "        self.solar_panel_area = 100  # Area covered by solar panels in square meters\n",
    "\n",
    "        # Define outside electricity cost\n",
    "        self.base_electricity_cost = 1  # Base cost per unit of electricity\n",
    "\n",
    "        # Initialize solar radiation parameters\n",
    "        self.solar_radiation_variation = 100  # Maximum variation in solar radiation\n",
    "        self.solar_radiation_mean = 700  # Mean solar radiation during the day\n",
    "        self.randomness_factor = 0.1  # Factor to control randomness\n",
    "\n",
    "        # Define the maximum number of people in each room\n",
    "        self.max_people_per_room = 3\n",
    "        self.num_people_in_rooms = self.num_people_per_bedroom.copy() + [0] * (num_bedrooms + 1)  # Initialize with specified people in bedrooms\n",
    "        self.num_people_in_rooms[-1] = num_people_in_living_room  # Set the number of people in the living room\n",
    "\n",
    "        # Define the frequency of adding/removing people (every 20 time steps)\n",
    "        self.add_remove_frequency = 25\n",
    "\n",
    "        # Exploration parameters\n",
    "        self.epsilon = 0.1 # Exploration rate\n",
    "        self.min_epsilon = 0.01  # Minimum exploration rate\n",
    "        self.epsilon_decay = 0.99  # Exploration decay rate\n",
    "\n",
    "    def get_solar_radiation(self, time_of_day):\n",
    "        # Simulate time-dependent solar radiation\n",
    "        # For simplicity, we'll assume a pattern with variations based on time of day\n",
    "\n",
    "        # Define solar radiation ranges for different times of the day\n",
    "        if time_of_day < 6:  # Night\n",
    "            solar_radiation = np.random.uniform(0, 100)\n",
    "        elif time_of_day < 9:  # Early morning\n",
    "            solar_radiation = np.random.uniform(500, 700)\n",
    "        elif time_of_day < 15:  # Afternoon\n",
    "            solar_radiation = np.random.uniform(700, 1000)\n",
    "        elif time_of_day < 19:  # Evening\n",
    "            solar_radiation = np.random.uniform(500, 700)\n",
    "        else:  # Night\n",
    "            solar_radiation = 0\n",
    "\n",
    "        # Introduce randomness with occasional complete randomness\n",
    "        if random.random() < self.randomness_factor:\n",
    "            solar_radiation = np.random.uniform(self.solar_radiation_mean - self.solar_radiation_variation,\n",
    "                                                 self.solar_radiation_mean + self.solar_radiation_variation)\n",
    "\n",
    "        return solar_radiation\n",
    "\n",
    "    def step(self, action):\n",
    "        temperature_action = action // 4 + 1  # Temperature adjustment\n",
    "        airflow_action = action % 4  # Airflow adjustment\n",
    "\n",
    "        # Determine the temperature change based on the action\n",
    "        temp_changes = np.zeros_like(self.temperatures)\n",
    "        temp_change = 0\n",
    "        if temperature_action == 1:  # Decrease temperature by 1 degree\n",
    "            temp_change = -1\n",
    "        elif temperature_action == 2:  # Keep temperature (no change)\n",
    "            temp_change = 0\n",
    "        elif temperature_action == 3:  # Increase temperature by 1 degree\n",
    "            temp_change = 1\n",
    "        elif temperature_action == 4:  # Decrease temperature by 2 degrees\n",
    "            temp_change = -2\n",
    "        elif temperature_action == 5:  # Increase temperature by 2 degrees\n",
    "            temp_change = 2\n",
    "        elif temperature_action == 6:  # Decrease temperature by 3 degrees\n",
    "            temp_change = -3\n",
    "        elif temperature_action == 7:  # Increase temperature by 3 degrees\n",
    "            temp_change = 3\n",
    "\n",
    "        # Apply the temperature change to all rooms\n",
    "        temp_changes[:self.num_bedrooms] = temp_change\n",
    "\n",
    "        # Clip temperature within the valid range\n",
    "        self.temperatures += temp_changes\n",
    "        self.temperatures = np.clip(self.temperatures, 16, 33)\n",
    "\n",
    "        # Calculate energy produced by solar panels\n",
    "        solar_radiation = self.get_solar_radiation(self.current_step / self.time_steps * 24)\n",
    "        solar_energy = self.solar_panel_area * self.solar_panel_efficiency * solar_radiation\n",
    "\n",
    "        # Calculate energy usage for temperature adjustment\n",
    "        energy_usage = np.abs(temp_changes) * self.energy_usage_factor\n",
    "\n",
    "        # Adjust airflow based on the action and number of people in the room\n",
    "        airflow = self.base_airflow + airflow_action * (self.max_airflow / (self.action_space.n // 4 - 1))\n",
    "        airflow_energy_usage = airflow * self.airflow_energy_consumption  # Initialize airflow energy usage\n",
    "        \n",
    "        # Adjust airflow energy consumption based on the number of people in each room\n",
    "        for room_idx, num_people in enumerate(self.num_people_in_rooms):\n",
    "            if num_people > 0:\n",
    "                airflow_energy_usage += self.airflow_energy_consumption * num_people\n",
    "\n",
    "        # Calculate net energy (energy usage - solar energy)\n",
    "        self.energy_level = min(self.energy_level + solar_energy, self.energy_capacity)\n",
    "        self.energy_level -= (np.sum(energy_usage) + airflow_energy_usage)\n",
    "\n",
    "        # Clip energy level within the valid range\n",
    "        self.energy_level = np.clip(self.energy_level, 0, self.energy_capacity)\n",
    "\n",
    "        # Calculate reward based on energy-saving state and temperature\n",
    "        # temperature_rewards = -0.5 * (np.abs(self.temperatures - self.ideal_temperatures) ** 2)\n",
    "        eps = 1e-6 \n",
    "        temperature_rewards = -np.abs(self.temperatures - self.ideal_temperatures) * np.log(np.abs(self.temperatures - self.ideal_temperatures + eps))\n",
    "        temperature_rewards_sum = 0.25*np.sum(temperature_rewards)\n",
    "\n",
    "        energy_reward = 1 if self.energy_level >= self.energy_threshold else -1\n",
    "\n",
    "        rewards = temperature_rewards_sum + energy_reward\n",
    "\n",
    "        # Check if the episode is done\n",
    "        done = self.current_step >= self.time_steps\n",
    "\n",
    "        # Increment the current time step\n",
    "        self.current_step += 1\n",
    "\n",
    "        # Add or remove people from the room every add_remove_frequency time steps\n",
    "        if self.current_step % self.add_remove_frequency == 0:\n",
    "            for i in range(self.num_bedrooms):\n",
    "                if self.num_people_in_rooms[i] < self.max_people_per_room:\n",
    "                    self.num_people_in_rooms[i] += 1\n",
    "                    break\n",
    "\n",
    "        # Exploration decay\n",
    "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "        # Choose action with ε-greedy exploration\n",
    "        if random.random() < self.epsilon:\n",
    "            action = self.action_space.sample()\n",
    "\n",
    "        # Additional info can be an empty dictionary\n",
    "        info = {}\n",
    "\n",
    "        # Calculate normalized observation\n",
    "        normalized_observation = np.hstack(((self.temperatures - 16) / 17, (self.energy_level - 0) / self.energy_capacity))\n",
    "        normalized_observation = normalized_observation.reshape((-1, 1))\n",
    "\n",
    "        # Return the next state, reward, whether the episode is done, and additional info\n",
    "        return normalized_observation, rewards, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the current time step\n",
    "        self.current_step = 0\n",
    "        \n",
    "        # Check if it's time for home cleaning\n",
    "        if self.current_step % self.cleaning_frequency == 0:\n",
    "            # Consume energy for home cleaning\n",
    "            self.energy_level -= self.cleaning_energy_consumption\n",
    "\n",
    "            # Clip energy level within the valid range\n",
    "            self.energy_level = max(0, min(self.energy_capacity, self.energy_level))\n",
    "\n",
    "        # Normalize observation space\n",
    "        normalized_observation = np.hstack(((self.temperatures - 16) / 17, (self.energy_level - 0) / self.energy_capacity))\n",
    "        normalized_observation = normalized_observation.reshape((-1, 1))\n",
    "\n",
    "        # Return the initial state\n",
    "        return normalized_observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnergySavingEnv_cont(gym.Env):\n",
    "    def __init__(self, num_bedrooms=3, num_people_per_bedroom=[1,2,1], num_people_in_living_room=2):\n",
    "        self.num_bedrooms = num_bedrooms\n",
    "        self.num_people_per_bedroom = num_people_per_bedroom if num_people_per_bedroom is not None else [2] * num_bedrooms\n",
    "        self.num_people_in_living_room = num_people_in_living_room\n",
    "\n",
    "        # Define the observation space with normalized ranges\n",
    "        num_rooms = num_bedrooms + 1  # Include living room as well\n",
    "        self.observation_space = spaces.Box(low=-1, high=1, shape=(num_rooms+1,1), dtype=np.float32)\n",
    "\n",
    "        # Define the initial temperature (random between 24 and 26 degrees Celsius) for each room\n",
    "        self.initial_temperatures = np.random.uniform(24, 26, size=(num_rooms,))\n",
    "\n",
    "        # Set the current temperature to the initial temperature\n",
    "        self.temperatures = self.initial_temperatures.copy()\n",
    "\n",
    "        # Initialize energy level for the whole house\n",
    "        self.energy_level = np.array([120])\n",
    "\n",
    "        # Define the ideal temperature for each room\n",
    "        self.ideal_temperatures = [22] * num_rooms  # Assuming ideal temperature of 22 for all rooms\n",
    "\n",
    "        # Define the resource manager properties\n",
    "        self.energy_capacity = 250\n",
    "        self.energy_threshold = 50  # Energy threshold for energy-saving mode\n",
    "        self.energy_usage_factor = 2  # Factor to control energy usage for temperature adjustment\n",
    "        self.base_airflow = 0  # Base airflow without energy consumption\n",
    "        self.max_airflow = 3  # Maximum airflow that can be achieved with energy consumption\n",
    "        self.airflow_energy_consumption = 0.5  # Energy consumption per unit increase in airflow\n",
    "\n",
    "        # Define the number of time steps for the resource manager\n",
    "        self.time_steps = 100\n",
    "\n",
    "        # Initialize the current time step\n",
    "        self.current_step = 0\n",
    "\n",
    "        self.cleaning_energy_consumption = 1  # Energy consumed during home cleaning\n",
    "\n",
    "        # Define the frequency of home cleaning (once a day/episode)\n",
    "        self.cleaning_frequency = 1\n",
    "\n",
    "        # Add solar energy parameters\n",
    "        self.solar_panel_efficiency = 0.2  # Efficiency of solar panels\n",
    "        self.solar_panel_area = 100  # Area covered by solar panels in square meters\n",
    "\n",
    "        # Define outside electricity cost\n",
    "        self.base_electricity_cost = 1  # Base cost per unit of electricity\n",
    "\n",
    "        # Initialize solar radiation parameters\n",
    "        self.solar_radiation_variation = 100  # Maximum variation in solar radiation\n",
    "        self.solar_radiation_mean = 700  # Mean solar radiation during the day\n",
    "        self.randomness_factor = 0.1  # Factor to control randomness\n",
    "\n",
    "        # Define the maximum number of people in each room\n",
    "        self.max_people_per_room = 3\n",
    "        self.num_people_in_rooms = self.num_people_per_bedroom.copy() + [0] * (num_bedrooms + 1)  # Initialize with specified people in bedrooms\n",
    "        self.num_people_in_rooms[-1] = num_people_in_living_room  # Set the number of people in the living room\n",
    "\n",
    "        # Define the frequency of adding/removing people (every 20 time steps)\n",
    "        self.add_remove_frequency = 25\n",
    "\n",
    "        # Exploration parameters\n",
    "        self.epsilon = 0.1 # Exploration rate\n",
    "        self.min_epsilon = 0.01  # Minimum exploration rate\n",
    "        self.epsilon_decay = 0.99  # Exploration decay rate\n",
    "\n",
    "        # Define the action space\n",
    "        self.action_space = spaces.Box(low=np.array([0, 0]), high=np.array([6, 3]), dtype=np.float32)  # Continuous action space: temperature adjustment and airflow adjustment\n",
    "\n",
    "    def get_solar_radiation(self, time_of_day):\n",
    "        # Simulate time-dependent solar radiation\n",
    "        # For simplicity, we'll assume a pattern with variations based on time of day\n",
    "\n",
    "        # Define solar radiation ranges for different times of the day\n",
    "        if time_of_day < 6:  # Night\n",
    "            solar_radiation = np.random.uniform(0, 100)\n",
    "        elif time_of_day < 9:  # Early morning\n",
    "            solar_radiation = np.random.uniform(500, 700)\n",
    "        elif time_of_day < 15:  # Afternoon\n",
    "            solar_radiation = np.random.uniform(700, 1000)\n",
    "        elif time_of_day < 19:  # Evening\n",
    "            solar_radiation = np.random.uniform(500, 700)\n",
    "        else:  # Night\n",
    "            solar_radiation = 0\n",
    "\n",
    "        # Introduce randomness with occasional complete randomness\n",
    "        if random.random() < self.randomness_factor:\n",
    "            solar_radiation = np.random.uniform(self.solar_radiation_mean - self.solar_radiation_variation,\n",
    "                                                 self.solar_radiation_mean + self.solar_radiation_variation)\n",
    "\n",
    "        return solar_radiation\n",
    "\n",
    "    def step(self, action):\n",
    "        temperature_action = int(action[0])  # Temperature adjustment\n",
    "        airflow_action = int(action[1])  # Airflow adjustment\n",
    "\n",
    "        # Determine the temperature change based on the action\n",
    "        temp_changes = np.zeros_like(self.temperatures)\n",
    "        temp_change = 0\n",
    "        if temperature_action == 0:  # Decrease temperature by 1 degree\n",
    "            temp_change = -1\n",
    "        elif temperature_action == 1:  # Keep temperature (no change)\n",
    "            temp_change = 0\n",
    "        elif temperature_action == 2:  # Increase temperature by 1 degree\n",
    "            temp_change = 1\n",
    "        elif temperature_action == 3:  # Decrease temperature by 2 degrees\n",
    "            temp_change = -2\n",
    "        elif temperature_action == 4:  # Increase temperature by 2 degrees\n",
    "            temp_change = 2\n",
    "        elif temperature_action == 5:  # Decrease temperature by 3 degrees\n",
    "            temp_change = -3\n",
    "        elif temperature_action == 6:  # Increase temperature by 3 degrees\n",
    "            temp_change = 3\n",
    "\n",
    "        # Apply the temperature change to all rooms\n",
    "        temp_changes[:self.num_bedrooms] = temp_change\n",
    "\n",
    "        # Clip temperature within the valid range\n",
    "        self.temperatures += temp_changes\n",
    "        self.temperatures = np.clip(self.temperatures, 16, 33)\n",
    "\n",
    "        # Calculate energy produced by solar panels\n",
    "        solar_radiation = self.get_solar_radiation(self.current_step / self.time_steps * 24)\n",
    "        solar_energy = self.solar_panel_area * self.solar_panel_efficiency * solar_radiation\n",
    "\n",
    "        # Calculate energy usage for temperature adjustment\n",
    "        energy_usage = np.abs(temp_changes) * self.energy_usage_factor\n",
    "\n",
    "        # Adjust airflow based on the action and number of people in the room\n",
    "        airflow = self.base_airflow + airflow_action * (self.max_airflow / (self.action_space.high[1] - self.action_space.low[1]))\n",
    "        airflow_energy_usage = airflow * self.airflow_energy_consumption  # Initialize airflow energy usage\n",
    "        \n",
    "        # Adjust airflow energy consumption based on the number of people in each room\n",
    "        for room_idx, num_people in enumerate(self.num_people_in_rooms):\n",
    "            if num_people > 0:\n",
    "                airflow_energy_usage += self.airflow_energy_consumption * num_people\n",
    "\n",
    "        # Calculate net energy (energy usage - solar energy)\n",
    "        self.energy_level = min(self.energy_level + solar_energy, self.energy_capacity)\n",
    "        self.energy_level -= (np.sum(energy_usage) + airflow_energy_usage)\n",
    "\n",
    "        # Clip energy level within the valid range\n",
    "        self.energy_level = np.clip(self.energy_level, 0, self.energy_capacity)\n",
    "\n",
    "        # Calculate reward based on energy-saving state and temperature\n",
    "        # temperature_rewards = -0.5 * (np.abs(self.temperatures - self.ideal_temperatures) ** 2)\n",
    "        eps = 1e-6 \n",
    "        temperature_rewards = -np.abs(self.temperatures - self.ideal_temperatures) * np.log(np.abs(self.temperatures - self.ideal_temperatures + eps))\n",
    "        temperature_rewards_sum = 0.25*np.sum(temperature_rewards)\n",
    "\n",
    "        energy_reward = 1 if self.energy_level >= self.energy_threshold else -1\n",
    "\n",
    "        rewards = temperature_rewards_sum + energy_reward\n",
    "\n",
    "        # Check if the episode is done\n",
    "        done = self.current_step >= self.time_steps\n",
    "\n",
    "        # Increment the current time step\n",
    "        self.current_step += 1\n",
    "\n",
    "        # Add or remove people from the room every add_remove_frequency time steps\n",
    "        if self.current_step % self.add_remove_frequency == 0:\n",
    "            for i in range(self.num_bedrooms):\n",
    "                if self.num_people_in_rooms[i] < self.max_people_per_room:\n",
    "                    self.num_people_in_rooms[i] += 1\n",
    "                    break\n",
    "\n",
    "        # Exploration decay\n",
    "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "        # Choose action with ε-greedy exploration\n",
    "        if random.random() < self.epsilon:\n",
    "            action = self.action_space.sample()\n",
    "\n",
    "        # Additional info can be an empty dictionary\n",
    "        info = {}\n",
    "\n",
    "        # Calculate normalized observation\n",
    "        normalized_observation = np.hstack(((self.temperatures - 16) / 17, (self.energy_level - 0) / self.energy_capacity))\n",
    "        normalized_observation = normalized_observation.reshape((-1, 1))\n",
    "\n",
    "        # Return the next state, reward, whether the episode is done, and additional info\n",
    "        return normalized_observation, rewards, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the current time step\n",
    "        self.current_step = 0\n",
    "        \n",
    "        # Check if it's time for home cleaning\n",
    "        if self.current_step % self.cleaning_frequency == 0:\n",
    "            # Consume energy for home cleaning\n",
    "            self.energy_level -= self.cleaning_energy_consumption\n",
    "\n",
    "            # Clip energy level within the valid range\n",
    "            self.energy_level = max(0, min(self.energy_capacity, self.energy_level))\n",
    "\n",
    "        # Normalize observation space\n",
    "        normalized_observation = np.hstack(((self.temperatures - 16) / 17, (self.energy_level - 0) / self.energy_capacity))\n",
    "        normalized_observation = normalized_observation.reshape((-1, 1))\n",
    "\n",
    "        # Return the initial state\n",
    "        return normalized_observation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manish/.local/lib/python3.10/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.9129391 ],\n",
       "       [-0.85357994],\n",
       "       [ 0.10297186],\n",
       "       [-0.10939454],\n",
       "       [ 0.5008115 ]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_energy_saving = EnergySavingEnv_cont()\n",
    "env_energy_saving.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.48903003],\n",
       "       [0.56885251],\n",
       "       [0.53444957],\n",
       "       [0.57436453],\n",
       "       [0.476     ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_energy_saving.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Score -546.1664087861998\n",
      "Episode: 2 Score -531.6861300454082\n"
     ]
    }
   ],
   "source": [
    "episodes = 2\n",
    "for episode in range(1, episodes+1):\n",
    "  obs = env_energy_saving.reset()\n",
    "  done = False\n",
    "  score = 0\n",
    "\n",
    "  while not done:\n",
    "    # env_energy_saving.render()\n",
    "    action = env_energy_saving.action_space.sample()\n",
    "    obs, reward, done, info = env_energy_saving.step(action)\n",
    "    score += reward\n",
    "\n",
    "  print('Episode: {} Score {}'.format(episode, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to train the model with different agents/models and compare the results obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join('Training', 'House_Logs') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# energy_saving_model = SAC('MlpPolicy', env_energy_saving, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# energy_saving_model.learn(total_timesteps=250000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = os.path.join('Training', 'Saved Models', f'SAC_250k_env_house')\n",
    "# energy_saving_model.save(path)\n",
    "# del energy_saving_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# energy_model = SAC.load(path, env_energy_saving)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_reward, std_reward = evaluate_policy(energy_model, env_energy_saving, n_eval_episodes=10, render=True)\n",
    "\n",
    "# print(f\"Mean reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# episodes = 5\n",
    "\n",
    "# for episode in range(1, episodes+1):\n",
    "#     obs = env_energy_saving.reset() \n",
    "#     done = False\n",
    "#     score = 0\n",
    "\n",
    "#     while not done:\n",
    "#         # obs = obs.reshape((1, -1))\n",
    "#         action, _ = energy_model.predict(obs)  \n",
    "#         obs, reward, done, info = env_energy_saving.step(action)\n",
    "#         score += reward\n",
    "\n",
    "#     print('Episode: {} Score {}'.format(episode, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_log_path = os.path.join(log_path, 'SAC_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard --logdir={training_log_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action-Critic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_saving_model = A2C('MlpPolicy', env_energy_saving, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_saving_model.learn(total_timesteps=250000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join('Training', 'Saved Models', f'A2C_250k_env_house')\n",
    "energy_saving_model.save(path)\n",
    "\n",
    "del energy_saving_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_model = A2C.load(path, env_energy_saving)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evalutaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(energy_model, env_energy_saving, n_eval_episodes=10, render=True)\n",
    "\n",
    "print(f\"Mean reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 5\n",
    "\n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env_energy_saving.reset() \n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        # obs = obs.reshape((1, -1))\n",
    "        action, _ = energy_model.predict(obs)  \n",
    "        obs, reward, done, info = env_energy_saving.step(action)\n",
    "        score += reward\n",
    "\n",
    "    print('Episode: {} Score {}'.format(episode, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log_path = os.path.join(log_path, 'A2C_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir={training_log_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_saving_model = PPO('MlpPolicy', env_energy_saving, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_saving_model.learn(total_timesteps=250000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join('Training', 'Saved Models', f'PPO_250k_env_house')\n",
    "energy_saving_model.save(path)\n",
    "\n",
    "del energy_saving_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_model = PPO.load(path, env_energy_saving)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evalutation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(energy_model, env_energy_saving, n_eval_episodes=10, render=True)\n",
    "\n",
    "print(f\"Mean reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 5\n",
    "\n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env_energy_saving.reset() \n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        # obs = obs.reshape((1, -1))\n",
    "        action, _ = energy_model.predict(obs)  \n",
    "        obs, reward, done, info = env_energy_saving.step(action)\n",
    "        score += reward\n",
    "\n",
    "    print('Episode: {} Score {}'.format(episode, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log_path = os.path.join(log_path, 'PPO_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir={training_log_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_energy_saving = EnergySavingEnv_Disc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_saving_model = DQN('MlpPolicy', env_energy_saving, verbose=1, tensorboard_log=log_path)\n",
    "energy_saving_model.learn(total_timesteps=250000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join('Training', 'Saved Models', f'DQN_250k_env_house')\n",
    "energy_saving_model.save(path)\n",
    "del energy_saving_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(energy_model, env_energy_saving, n_eval_episodes=10, render=True)\n",
    "print(f\"Mean reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "episodes = 5\n",
    "\n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env_energy_saving.reset() \n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        # obs = obs.reshape((1, -1))\n",
    "        action, _ = energy_model.predict(obs)  \n",
    "        obs, reward, done, info = env_energy_saving.step(action)\n",
    "        score += reward\n",
    "\n",
    "    print('Episode: {} Score {}'.format(episode, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log_path = os.path.join(log_path, 'DQN_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir={training_log_path}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
