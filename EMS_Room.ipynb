{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "from stable_baselines3 import DQN,A2C,PPO\n",
    "\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives:\n",
    "\n",
    "The objective of the project is to design, train and evaluate a framework for an agent in a low-energy environment. It seeks to efficiently manage the heat and air in the room to reduce energy consumption and improve occupant comfort. \n",
    "\n",
    "Leveraging reinforcement learning, the goal is to train agents to dynamically adjust controls based on factors such as occupancy, solar radiation, energy availability, and so on. \n",
    "\n",
    "Ultimately, the project aims to help achieve optimal energy without compromising comfort, as likewise sustainable building design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "import gym.spaces as spaces\n",
    "\n",
    "class EnergySavingEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        # Define the action space\n",
    "        self.action_space = spaces.Discrete(28)  # Total number of actions: 7 temperature adjustments * 4 airflow adjustments\n",
    "        \n",
    "        # Define the observation space with normalized ranges\n",
    "        self.observation_space = spaces.Box(low=np.array([-1, -1]), high=np.array([1, 1]), dtype=np.float32)  # Normalized observation space: temperature and energy level\n",
    "\n",
    "        # Define the initial temperature (random between 24 and 26 degrees Celsius)\n",
    "        self.initial_temperature = np.random.uniform(24, 26)\n",
    "\n",
    "        # Set the current temperature to the initial temperature\n",
    "        self.temperature = self.initial_temperature\n",
    "\n",
    "        # Initialize energy level\n",
    "        self.energy_level = 60\n",
    "\n",
    "        # Define the ideal temperature\n",
    "        self.ideal_temperature = 22\n",
    "\n",
    "        # Define the resource manager properties\n",
    "        self.energy_capacity = 100\n",
    "        self.energy_threshold = 30  # Energy threshold for energy-saving mode\n",
    "        self.energy_usage_factor = 2  # Factor to control energy usage for temperature adjustment\n",
    "        self.base_airflow = 0  # Base airflow without energy consumption\n",
    "        self.max_airflow = 3  # Maximum airflow that can be achieved with energy consumption\n",
    "        self.airflow_energy_consumption = 5  # Energy consumption per unit increase in airflow\n",
    "\n",
    "        # Define the number of time steps for the resource manager\n",
    "        self.time_steps = 100\n",
    "\n",
    "        # Initialize the current time step\n",
    "        self.current_step = 0\n",
    "\n",
    "        self.cleaning_energy_consumption = 5  # Energy consumed during home cleaning\n",
    "\n",
    "        # Define the frequency of home cleaning (once a day/episode)\n",
    "        self.cleaning_frequency = 1\n",
    "\n",
    "        # Add solar energy parameters\n",
    "        self.solar_panel_efficiency = 0.18  # Efficiency of solar panels\n",
    "        self.solar_panel_area = 25  # Area covered by solar panels in square meters\n",
    "\n",
    "        # Define outside electricity cost\n",
    "        self.base_electricity_cost = 1  # Base cost per unit of electricity\n",
    "\n",
    "        # Initialize solar radiation parameters\n",
    "        self.solar_radiation_variation = 100  # Maximum variation in solar radiation\n",
    "        self.solar_radiation_mean = 700  # Mean solar radiation during the day\n",
    "        self.randomness_factor = 0.1  # Factor to control randomness\n",
    "\n",
    "        # Define the maximum number of people in the room\n",
    "        self.max_people = 5\n",
    "\n",
    "        # Initialize the number of people in the room\n",
    "        self.num_people = 0\n",
    "\n",
    "        # Define the frequency of adding/removing people (every 20 time steps)\n",
    "        self.add_remove_frequency = 25\n",
    "\n",
    "        # Exploration parameters\n",
    "        self.epsilon = 0.1  # Exploration rate\n",
    "        self.min_epsilon = 0.01  # Minimum exploration rate\n",
    "        self.epsilon_decay = 0.99  # Exploration decay rate\n",
    "\n",
    "    def get_solar_radiation(self, time_of_day):\n",
    "        # Simulate time-dependent solar radiation\n",
    "        # For simplicity, we'll assume a pattern with variations based on time of day\n",
    "\n",
    "        # Define solar radiation ranges for different times of the day\n",
    "        if time_of_day < 6:  # Night\n",
    "            solar_radiation = np.random.uniform(0, 100)\n",
    "        elif time_of_day < 9:  # Early morning\n",
    "            solar_radiation = np.random.uniform(500, 700)\n",
    "        elif time_of_day < 15:  # Afternoon\n",
    "            solar_radiation = np.random.uniform(700, 1000)\n",
    "        elif time_of_day < 19:  # Evening\n",
    "            solar_radiation = np.random.uniform(500, 700)\n",
    "        else:  # Night\n",
    "            solar_radiation = 0\n",
    "\n",
    "        # Introduce randomness with occasional complete randomness\n",
    "        if random.random() < self.randomness_factor:\n",
    "            solar_radiation = np.random.uniform(self.solar_radiation_mean - self.solar_radiation_variation,\n",
    "                                                 self.solar_radiation_mean + self.solar_radiation_variation)\n",
    "\n",
    "        return solar_radiation\n",
    "\n",
    "    def step(self, action):\n",
    "        temperature_action = action // 4 + 1  # Temperature adjustment\n",
    "        airflow_action = action % 4  # Airflow adjustment\n",
    "\n",
    "        # Determine the temperature change based on the action\n",
    "        temp_change = 0\n",
    "        if temperature_action == 1:  # Decrease temperature by 1 degree\n",
    "            temp_change = -1\n",
    "        elif temperature_action == 2:  # Keep temperature (no change)\n",
    "            temp_change = 0\n",
    "        elif temperature_action == 3:  # Increase temperature by 1 degree\n",
    "            temp_change = 1\n",
    "        elif temperature_action == 4:  # Decrease temperature by 2 degrees\n",
    "            temp_change = -2\n",
    "        elif temperature_action == 5:  # Increase temperature by 2 degrees\n",
    "            temp_change = 2\n",
    "        elif temperature_action == 6:  # Decrease temperature by 3 degrees\n",
    "            temp_change = -3\n",
    "        elif temperature_action == 7:  # Increase temperature by 3 degrees\n",
    "            temp_change = 3\n",
    "\n",
    "        # Apply the temperature change\n",
    "        self.temperature += temp_change\n",
    "\n",
    "        # Clip temperature within the valid range\n",
    "        self.temperature = max(16, min(33, self.temperature))\n",
    "\n",
    "        # Calculate energy produced by solar panels\n",
    "        solar_radiation = self.get_solar_radiation(self.current_step / self.time_steps * 24)\n",
    "        solar_energy = self.solar_panel_area * self.solar_panel_efficiency * solar_radiation\n",
    "\n",
    "        # Calculate energy usage for temperature adjustment\n",
    "        energy_usage = (abs(temp_change)) * self.energy_usage_factor\n",
    "\n",
    "        # Adjust airflow based on the action and number of people in the room\n",
    "        airflow = self.base_airflow + airflow_action * (self.max_airflow / (self.action_space.n // 4 - 1))\n",
    "        airflow_energy_usage = 0  # Initialize airflow energy usage\n",
    "        \n",
    "        # Adjust airflow energy consumption based on the number of people in the room\n",
    "        if self.num_people > 0:\n",
    "            airflow_energy_usage = self.airflow_energy_consumption * self.num_people\n",
    "\n",
    "        # Calculate net energy (energy usage - solar energy)\n",
    "        self.energy_level = min(self.energy_level + solar_energy, self.energy_capacity)\n",
    "        self.energy_level -= (energy_usage + airflow_energy_usage)\n",
    "\n",
    "        # Clip energy level within the valid range\n",
    "        self.energy_level = max(0, min(self.energy_capacity, self.energy_level))\n",
    "\n",
    "        # Calculate reward based on energy-saving state and temperature\n",
    "        temperature_reward = -0.5 * (abs(self.temperature - self.ideal_temperature) ** 2)\n",
    "\n",
    "        if self.energy_level >= self.energy_threshold:\n",
    "            energy_reward = 1  # Reward for maintaining energy above threshold\n",
    "        else:\n",
    "            energy_reward = -1\n",
    "\n",
    "        reward = temperature_reward + energy_reward\n",
    "\n",
    "        # Check if the episode is done\n",
    "        done = self.current_step >= self.time_steps\n",
    "\n",
    "        # Increment the current time step\n",
    "        self.current_step += 1\n",
    "\n",
    "        # Add or remove people from the room every 20 time steps\n",
    "        if self.current_step % self.add_remove_frequency == 0:\n",
    "            if random.random() < 0.5:  # Randomly decide whether to add or remove people\n",
    "                if self.num_people < self.max_people:\n",
    "                    self.num_people += 1\n",
    "            else:\n",
    "                if self.num_people > 0:\n",
    "                    self.num_people -= 1\n",
    "\n",
    "        # Exploration decay\n",
    "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "        # Choose action with Îµ-greedy exploration\n",
    "        if random.random() < self.epsilon:\n",
    "            action = self.action_space.sample()\n",
    "\n",
    "        # Additional info can be an empty dictionary\n",
    "        info = {}\n",
    "\n",
    "        # Normalize observation space\n",
    "        normalized_observation = np.array([(self.temperature - 16) / 17, self.energy_level / self.energy_capacity])\n",
    "\n",
    "        # Return the next state, reward, whether the episode is done, and additional info\n",
    "        return normalized_observation, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the current time step\n",
    "        self.current_step = 0\n",
    "        \n",
    "        # Randomly generate the number of people in the room\n",
    "        self.num_people = random.randint(0, self.max_people)\n",
    "\n",
    "        # Check if it's time for home cleaning\n",
    "        if self.current_step % self.cleaning_frequency == 0:\n",
    "            # Consume energy for home cleaning\n",
    "            self.energy_level -= self.cleaning_energy_consumption\n",
    "\n",
    "            # Clip energy level within the valid range\n",
    "            self.energy_level = max(0, min(self.energy_capacity, self.energy_level))\n",
    "\n",
    "        # Normalize observation space\n",
    "        normalized_observation = np.array([(self.temperature - 16) / 17, self.energy_level / self.energy_capacity])\n",
    "\n",
    "        # Return the initial state\n",
    "        return normalized_observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_energy_saving = EnergySavingEnv()\n",
    "env_energy_saving.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_energy_saving.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 2\n",
    "for episode in range(1, episodes+1):\n",
    "  obs = env_energy_saving.reset()\n",
    "  done = False\n",
    "  score = 0\n",
    "\n",
    "  while not done:\n",
    "    # env_energy_saving.render()\n",
    "    action = env_energy_saving.action_space.sample()\n",
    "    obs, reward, done, info = env_energy_saving.step(action)\n",
    "    score += reward\n",
    "\n",
    "  print('Episode: {} Score {}'.format(episode, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to train the model with different agents/models and compare the results obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join('Training', 'Logs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_saving_model = DQN('MlpPolicy', env_energy_saving, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_saving_model.learn(total_timesteps=250000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join('Training', 'Saved Models', f'DQN_250k_env_room')\n",
    "energy_saving_model.save(path)\n",
    "del energy_saving_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_model = DQN.load(path, env_energy_saving)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(energy_model, env_energy_saving, n_eval_episodes=10, render=True)\n",
    "\n",
    "print(f\"Mean reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 5\n",
    "\n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env_energy_saving.reset() \n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        # obs = obs.reshape((1, -1))\n",
    "        action, _ = energy_model.predict(obs)  \n",
    "        obs, reward, done, info = env_energy_saving.step(action)\n",
    "        score += reward\n",
    "\n",
    "    print('Episode: {} Score {}'.format(episode, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action-Critic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_saving_model = A2C('MlpPolicy', env_energy_saving, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_saving_model.learn(total_timesteps=250000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join('Training', 'Saved Models', f'A2C_250k_env_room')\n",
    "energy_saving_model.save(path)\n",
    "\n",
    "del energy_saving_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_model = A2C.load(path, env_energy_saving)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evalutaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(energy_model, env_energy_saving, n_eval_episodes=10, render=True)\n",
    "\n",
    "print(f\"Mean reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 5\n",
    "\n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env_energy_saving.reset() \n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        # obs = obs.reshape((1, -1))\n",
    "        action, _ = energy_model.predict(obs)  \n",
    "        obs, reward, done, info = env_energy_saving.step(action)\n",
    "        score += reward\n",
    "\n",
    "    print('Episode: {} Score {}'.format(episode, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_saving_model = PPO('MlpPolicy', env_energy_saving, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_saving_model.learn(total_timesteps=250000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join('Training', 'Saved Models', f'PPO_250k_env_room')\n",
    "energy_saving_model.save(path)\n",
    "\n",
    "del energy_saving_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_model = PPO.load(path, env_energy_saving)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evalutation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(energy_model, env_energy_saving, n_eval_episodes=10, render=True)\n",
    "\n",
    "print(f\"Mean reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 5\n",
    "\n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env_energy_saving.reset() \n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        # obs = obs.reshape((1, -1))\n",
    "        action, _ = energy_model.predict(obs)  \n",
    "        obs, reward, done, info = env_energy_saving.step(action)\n",
    "        score += reward\n",
    "\n",
    "    print('Episode: {} Score {}'.format(episode, score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
